# Slip Happens: How Large Language Models Deal With Misspellings

**Author**: Ida Caspary (Imperial College London)  
**Paper**: *"How Large Language Models Deal With Misspellings"*  
**Colab Notebook**: [Link](https://colab.research.google.com/drive/1AzfSVcl8XVs6IPQD5RVKr43oYUZ9p3qN?usp=sharing)  
**GitHub**: [https://github.com/IdaCy/mech-typo-detect](https://github.com/IdaCy/mech-typo-detect)

This repository contains code and data for analyzing how a 7B-parameter Mistral model handles typographical errors. The accompanying paper explores the internal hidden states, focusing on how a single dimension or small subspace can encode “typo anomalies” and facilitate LLM resilience. In particular, neuron **#2070** in layer **2** stands out as highly sensitive to spelling distortions.

---

## Project Structure

- **analyses_results**  
  Stores outputs from PCA, neuron-dissection, and hooking experiments (e.g., `.pt` files with difference vectors, CSV logs of ablation runs, and final results or plots).
  
- **containers**  
  Holds any containerization or environment-setup files, if used.

- **extractions**  
  Contains `.pt` dumps of hidden states, attention scores, and logits for each input prompt pair (clean vs. typo). Generated by the inference scripts.

- **logs**  
  Contains `.log` files capturing verbose runtime information from hooking or ablation scripts.

- **scripts**  
  - **mistral/** includes the primary inference scripts.  
  - **analyses/** focuses on extracting and comparing hidden-state differences, running PCA, and computing explained variances.  
  - **investigations/** contains deeper neuron-level or subspace-level explorations, such as hooking and correlation tests.

The paper itself  explains all relevant experiments in detail— from building a 5000-pair dataset of clean vs. typo prompts, to using PCA on the difference-of-activations, to hooking neurons or entire dimensions in the model.

---

## Usage

1. **Colab Notebook**  
   The quickest way to reproduce key results is via [the Google Colab link](https://colab.research.google.com/drive/1AzfSVcl8XVs6IPQD5RVKr43oYUZ9p3qN?usp=sharing). It walks through loading the data, running PCA, analyzing neuron #2070, and performing hooking experiments.

2. **Local or HPC Environment**  
   - Clone this repo:  
     ```
     git clone https://github.com/IdaCy/mech-typo-detect.git
     cd mech-typo-detect
     ```  
   - Ensure Python 3.9+ or similar, plus PyTorch, Transformers, NumPy, Matplotlib, scikit-learn, etc.
   - Place your `cleanQs.csv` and `typoQs.csv` prompts in `prompts/preprocessed/` (or adjust paths in the scripts).
   - Run inference (to generate `.pt` activation files) from `scripts/mistral/`.  
   - Proceed with difference computation, PCA, or hooking by calling scripts in `scripts/analyses/` and `scripts/investigations/`.

3. **Outputs & Logs**  
   - Intermediate `.pt` files end up in `extractions/` or `analyses_results/`.  
   - PCA results, hooking outcomes, and correlation data are stored in `analyses_results/` subdirectories.  
   - Console messages also appear in `.log` files within `logs/`.

---

## Paper Abstract

> *“Slip Happens: How Large Language Models Deal With Misspellings”*  
> This study analyzes the internal representations of a Mistral 7B model, showing that a near–rank‐1 subspace tracks typographical errors from early to mid layers. One neuron (#2070) drives a consistent activation shift, and ablating it or removing the associated dimension slightly reduces model performance and stability, suggesting a low‐dimensional mechanism for handling user spelling slips.

See the full text in this repo (or in the [Colab Notebook](https://colab.research.google.com/drive/1AzfSVcl8XVs6IPQD5RVKr43oYUZ9p3qN?usp=sharing)) for details, methodology, and references.

---

## Contact & License

This work is part of a research project at **Imperial College London**. Please open an issue or pull request on GitHub if you find any problems or have ideas to extend the codebase. The repository is made available under an MIT License unless otherwise noted.
